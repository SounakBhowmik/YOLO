{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/18653/Documents/School/COSC 525/Final Project/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:862: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/18653/Documents/School/COSC 525/Final Project/venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, torch, torch._dynamo\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from titans_pytorch import MemoryAsContextTransformer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# ── auth / model ────────────────────────────────────────────────────────────────\n",
    "load_dotenv();  login(os.getenv(\"ACCESS_TOKEN\"))\n",
    "model_id   = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "llm        = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16, device_map=device, use_auth_token=True\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── memory transformer ─────────────────────────────────────────────────────────\n",
    "mem_tx = MemoryAsContextTransformer(\n",
    "    num_tokens=len(tokenizer),\n",
    "    dim=llm.config.hidden_size,\n",
    "    depth=2,\n",
    "    segment_len=128,\n",
    "    neural_memory_segment_len=16,\n",
    "    num_persist_mem_tokens=64,\n",
    "    num_longterm_mem_tokens=128,\n",
    "    neural_memory_kwargs=dict(\n",
    "        heads=8, dim_head=256, pre_rmsnorm=True, post_rmsnorm=True,\n",
    "        qk_rmsnorm=True, attn_pool_chunks=True, momentum=True, momentum_order=1,\n",
    "    ),\n",
    "    use_flex_attn=True,\n",
    "    sliding_window_attn=False,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ── detect objects in video ────────────────────────────────────────────────────\n",
    "cap   = cv2.VideoCapture(\"zebra.mp4\")\n",
    "yolo  = YOLO(\"yolov8n.pt\")\n",
    "sent  = []\n",
    "f_idx = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    f_idx += 1\n",
    "    res = yolo(frame, verbose=False)[0]\n",
    "    objs = {yolo.model.names[int(c)] for c in res.boxes.cls}\n",
    "    sent.append(f\"Frame {f_idx}: \" + (\", \".join(sorted(objs)) if objs else \"no objects detected\"))\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# ── build doc from detections ──────────────────────────────────────────────────\n",
    "doc    = \". \".join(sent) + \".\"\n",
    "chunks = [s.strip() + \".\" for s in doc.split(\". \") if s]\n",
    "chunk_embs = []\n",
    "\n",
    "for ch in chunks:\n",
    "    ids  = tokenizer(ch, return_tensors=\"pt\").input_ids.to(device)\n",
    "    loss = mem_tx(ids, return_loss=True)\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = mem_tx.token_emb(ids).mean(dim=1).squeeze(0)\n",
    "        chunk_embs.append(emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── document ────────────────────────────────────────────────────────────\n",
    "doc = \"Artificial intelligence (AI) is rapidly transforming the modern world. From personalized recommendations on streaming platforms to autonomous vehicles navigating complex environments, AI technologies are embedded into the fabric of daily life. Businesses are increasingly investing in AI to streamline operations, gain insights from data, and improve customer experiences. This shift is not only technological but also cultural, as organizations adapt to new ways of thinking and working. A key driver of this transformation is machine learning (ML), which allows systems to learn from data and improve over time without explicit programming. Among the most impactful ML techniques are deep learning and reinforcement learning. These methods have led to breakthroughs in fields such as computer vision, natural language processing, and robotics. However, they also introduce new challenges, such as the need for vast amounts of training data and computational resources. Ethical concerns surrounding AI are also growing. Issues such as algorithmic bias, lack of transparency, and potential job displacement are prompting discussions among policymakers, researchers, and the public. Efforts are underway to create regulatory frameworks and ethical guidelines to ensure AI is developed and deployed responsibly. Transparency, fairness, and accountability are emerging as key principles in the field. Despite the challenges, the future of AI holds great promise. Innovations in hardware, such as neuromorphic chips, and advances in software, like transformer-based models, are pushing the boundaries of what AI systems can achieve. As interdisciplinary collaboration continues to grow, AI is likely to become even more integral to solving complex global problems in healthcare, climate science, education, and beyond.\"\n",
    "chunks = [s.strip() + \".\" for s in doc.split(\". \") if s]\n",
    "chunk_embs = []\n",
    "\n",
    "for ch in chunks:\n",
    "    ids  = tokenizer(ch, return_tensors=\"pt\").input_ids.to(device)\n",
    "    loss = mem_tx(ids, return_loss=True)\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = mem_tx.token_emb(ids).mean(dim=1).squeeze(0)   # (dim,)\n",
    "        chunk_embs.append(emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a monkey. \n",
      "\n",
      "Frame 301: no objects detected.\n",
      "\n",
      "Frame 171: no objects detected.\n",
      "\n",
      "Frame 265: bench.\n",
      "\n",
      "### Explanation\n",
      "In Frame 301, no objects are detected, which means the environment is empty. The question is then asked about the animal that this environment contains. Since no objects are detected, the answer is not based on the objects themselves, but rather on the fact that the environment is empty. In this case, the answer is a monkey, as monkeys are typically found in trees or other elevated environments.\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def summarise(question: str,\n",
    "              *,\n",
    "              k: int = 3,\n",
    "              max_new_tokens: int = 128,\n",
    "              temperature: float = 0.7) -> str:\n",
    "    \"\"\"Retrieve top-k relevant chunks and let LLaMA answer.\"\"\"\n",
    "    # ---- embed the question in the same space -------------------------------\n",
    "    q_ids = tokenizer(question,\n",
    "                      add_special_tokens=False,\n",
    "                      return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        q_emb = mem_tx.token_emb(q_ids).mean(dim=1)          # (1, dim)\n",
    "\n",
    "        # ---- similarity against all chunk embeddings ------------------------\n",
    "        sims = torch.stack([\n",
    "            cosine_similarity(q_emb, emb.unsqueeze(0), dim=-1)[0]\n",
    "            for emb in chunk_embs\n",
    "        ])                                                    # (num_chunks,)\n",
    "\n",
    "        topk = sims.topk(min(k, len(chunks))).indices.tolist()\n",
    "\n",
    "    # ---- build text prompt for LLaMA ----------------------------------------\n",
    "    retrieved = \"\\n\\n\".join(chunks[i] for i in topk)\n",
    "    prompt = f\"\"\"### Context\n",
    "{retrieved}\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "### Answer\n",
    "\"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = llm.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    completion = out_ids[0][input_ids.shape[1]:]\n",
    "    return tokenizer.decode(completion, skip_special_tokens=True).strip()\n",
    "\n",
    "print(summarise(\"what animal is this?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A simple stop sign with a black and white design. The sign has a circular frame with a white background. The frame contains a black and white image of a zebra. The image is centered and the zebra is shown in a simple pose.', '\"Stop sign and zebra. Stop sign and zebra. Stop sign and zebra. Stop sign and zebra. Stop sign and zebra. Stop sign and zebra. Stop sign and zebra. Stop sign and zebra. Stop', '\"Frame 1: stop sign, zebra. Frame 2: stop sign, zebra. Frame 3: stop sign, zebra. Frame 4: stop sign, zebra. Frame 5: stop sign, zebra', 'A person is standing next to a stop sign with a zebra on it. The stop sign is flanked by two zebra symbols. The caption reads \"A classic combination: a person and a stop sign with a zebra on it. A', '\"Stop signs and zebras are the best friends anyone could ask for. They\\'re always together, and they\\'re always on the same team. They may not be able to speak to each other, but they can certainly talk to us.\"', 'The image is of a stop sign with a zebra. The caption reads: \"The stop sign is the most common traffic control device in the world. It is used to stop vehicles from entering a certain area. The zebra is a popular animal', '\"Stop Sign, Zebra\"\\n\\n### Source\\n\"Frame 1: stop sign, zebra. Frame 2: stop sign, zebra. Frame 3: stop sign, zebra. Frame 4: stop sign, zebra', 'A stop sign with a zebra on it.\\n\\n### Image\\nA stop sign with a zebra on it.\\n\\n### Code\\n```python\\nimport matplotlib.pyplot as plt\\n\\n# Create a figure and a set of subplots\\nfig, ax =', '\"Zebra and Frisbee: A Perfect Pairing\"\\n\\nThis image features a zebra and a frisbee together, showcasing the perfect pairing of these two popular items. The zebra, with its striking black and white stripes, is perfectly', 'A zebra and a frisbee are standing on a grassy field, with a stop sign in the distance. The zebra is wearing a sports jersey, and the frisbee has a sports ball on it. The caption reads \"A', 'Frame 1: A person is standing in front of a stop sign with a zebra on the ground. The person is holding a frisbee.\\n\\nFrame 2: A person is standing in front of a stop sign with a zebra on', 'The image is a picture of a person and a zebra standing side by side. The person is holding a sports ball in one hand and a frisbee in the other. The zebra is looking directly at the camera, and the person is', 'Frame 1: stop sign, zebra. Frame 2: stop sign, zebra. Frame 3: stop sign, zebra. Frame 4: stop sign, zebra. Frame 5: stop sign, zebra.', '\"Frame 1: Stop sign, zebra. Frame 2: Stop sign, zebra. Frame 3: Stop sign, zebra. Frame 4: Stop sign, zebra. Frame 5: Stop sign, zebra']\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "captions = []\n",
    "for end in range(60, len(sent) + 1, 60):\n",
    "    # build context from frames 1..end\n",
    "    context = \". \".join(sent[:end]) + \".\"\n",
    "    prompt = f\"\"\"### Context\n",
    "{context}\n",
    "\n",
    "### Caption\n",
    "\"\"\"\n",
    "    # tokenize and generate\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = llm.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    # decode and store the caption\n",
    "    caption = tokenizer.decode(out_ids[0][input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "    captions.append(caption)\n",
    "\n",
    "print(captions)\n",
    "\n",
    "# reopen video and prepare writer\n",
    "cap2 = cv2.VideoCapture(\"zebra.mp4\")\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "fps   = cap2.get(cv2.CAP_PROP_FPS)\n",
    "w     = int(cap2.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h     = int(cap2.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out   = cv2.VideoWriter(\"zebra_with_captions.mp4\", fourcc, fps, (w, h))\n",
    "\n",
    "frame_idx = 0\n",
    "while True:\n",
    "    ret, frame = cap2.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # pick caption based on which 60-frame segment\n",
    "    seg = min(frame_idx // 100, len(captions) - 1)\n",
    "    text = captions[seg]\n",
    "\n",
    "    # wrap the caption to fit the frame width\n",
    "    max_chars_per_line = 20 \n",
    "    wrapped_lines = textwrap.wrap(text, width=max_chars_per_line)\n",
    "\n",
    "    # overlay each line at the bottom, stacking upwards\n",
    "    line_height = 45  # vertical space per line\n",
    "    for i, line in enumerate(reversed(wrapped_lines)):\n",
    "        y = h - 20 - i * line_height\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            line,\n",
    "            (10, y),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1.5,\n",
    "            (255, 255, 255),\n",
    "            2,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "\n",
    "    out.write(frame)\n",
    "    frame_idx += 1\n",
    "\n",
    "cap2.release()\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
